# -*- coding: utf-8 -*-
"""Main_RF & XG_ML Project_GPay_offers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16AcCUKGHi-V2I0LMfJzhip4F1MFWRhIf

# **Personalized Offers and Rewards recommendation for GPay Users**


This project aims to develop a machine learning-based personalized rewards and offers system for GPay users.

The system analyzes transaction history, spending patterns, and user engagement to predict and recommend tailored offers, improving user engagement and merchant satisfaction.

Engineered features, built predictive models,

and optimized accuracy using hyperparameter tuning. Deploy the model

The proposed solution uses advanced data analytics and machine learning models to provide recommendations that align with user preferences and spending habits. By understanding user behavior, the system can deliver the right offers at the right time, increasing offer redemption rates and overall user satisfaction.

##  Offer_Redeemed as a target variable

Supervise Learning -- if we have labelled data

#Importing Required Libraries
"""

import pandas as pd
import numpy as np

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split

from imblearn.over_sampling import SMOTE
from sklearn.model_selection import  StratifiedKFold, GridSearchCV

from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix

import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/bank_transactions_data.csv")
df.shape

print(df.head())
print(df.info())

#find all distinct Account ID from df
df['AccountID'].unique()
#df['AccountID'].nunique()

df['MerchantID'].unique()

"""#Step1:Data Preprocessing"""

df.isnull().sum()

df

df = df.drop(["CustomerAge",	"gender","PreviousTransactionDate"], axis=1)

df["TransactionDate"] = pd.to_datetime(df["TransactionDate"], format="%d-%m-%Y %H:%M")

#df['Latest_TransationDate'] = df["TransactionDate"].max()
#finding the latest transaction per user account
df['Last_TransactionDate'] = df.groupby('AccountID')['TransactionDate'].transform('max')
df

#To show monthly number of transaction of account
df['TransactionDate'] = pd.to_datetime(df['TransactionDate'])
df['TransactionMonth'] = df['TransactionDate'].dt.to_period('M')
monthly_transactions = df.groupby('AccountID')['TransactionMonth'].value_counts().unstack(fill_value=0)
monthly_transactions

#to show Total number of transactions made by each account
account_transactions = df.groupby('AccountID').size()
account_transactions

"""#Step2: Feature Engg"""

# Compute time difference (days since last transaction)
df["DaysSinceLastTransaction"] = (df["Last_TransactionDate"] - df["TransactionDate"]).dt.days
df.columns

# Aggregate user features
user_features = df.groupby("AccountID").agg(
    MostFrequentCategory=("transaction_category_last", lambda x: x.mode()[0] if not x.mode().empty else "Unknown"),
    MostFrequentMerchant=("MerchantID", lambda x: x.mode()[0] if not x.mode().empty else "Unknown"),
    AvgDaysBetweenTransactions=("DaysSinceLastTransaction", "mean"),
    offer_acceptance_rate=('offer_accepted', 'mean')
).reset_index()

# Merge back with original dataset
df = df.merge(user_features, on="AccountID", how="left")
df.head()
df.columns

# Aggregate user spending behavior
spending_features = df.groupby("AccountID").agg(
    TotalSpend=("TransactionAmount", "sum"),
    AvgTransactionAmount=("TransactionAmount", "mean"),
    TransactionCount=("TransactionID", "count"),
).reset_index()

# Define spending categories based on quartiles
spending_features["SpendingCategory"] = pd.qcut(
    spending_features["TotalSpend"], q=4, labels=["Low", "Medium", "High", "Premium"]
)
# Merge back with main dataframe
df = df.merge(spending_features, on="AccountID", how="left")
df.head()
df.columns

# Encode categorical features
categorical_cols = ["MostFrequentCategory", "MostFrequentMerchant", "offer_type_last_given","SpendingCategory"]
label_encoders = {col: LabelEncoder() for col in categorical_cols}

for col in categorical_cols:
    df[col] = label_encoders[col].fit_transform(df[col])

df.head(15)

# Select features and target variable

features = ["TransactionCount", "TotalSpend", "TransactionAmount", "offer_acceptance_rate", "SpendingCategory"]


X = df[features]
y = df["offer_accepted"]

# Standardize numerical features & split train - test data
"""scaler = StandardScaler()
X[["TransactionCount",
   "AvgDaysBetweenTransactions"]] = scaler.fit_transform(
    X[["TransactionCount",     "AvgDaysBetweenTransactions"]])"""

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Display dataset shape
X_train.shape, X_test.shape

"""#Step3: Model Selection with default parameters"""

#1)Random forest

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Initialize the Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

# Train the model
rf_model.fit(X_train, y_train)

# Predict on test data
y_pred_rf = rf_model.predict(X_test)

# Evaluate the model
accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Random Forest Accuracy: {accuracy_rf}")

# Classification report
print("Classification Report:\n", classification_report(y_test, y_pred_rf))

#Calculate Training accuracy with random forest
y_train_pred = rf_model.predict(X_train)
accuracy_train = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {accuracy_train}")

#2)Model implementation - XGBoost Algorithm
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, classification_report
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train, y_train)

y_pred_xg = xgb_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred_xg)
print(f"Accuracy: {accuracy}")

# Classification report
print("Classification Report:\n", classification_report(y_test, y_pred_xg))

#Calculate Training accuracy with XGBoost
y_train_pred = xgb_model.predict(X_train)
accuracy_train = accuracy_score(y_train, y_train_pred)
print(f"Training Accuracy: {accuracy_train}")

"""===========================================================================================

#Step4: Hyperparameter Tunning with RF & XGBOOST

#resample data to handle imbancing in class SMOTE & GridSearchCV

#Step 5: Evaluation Accuracy
"""

from imblearn.over_sampling import SMOTE
# Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Split data
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)

"""#PART I : RF"""

from sklearn.model_selection import  StratifiedKFold, GridSearchCV

# Model training - RandomForest with GridSearchCV
param_grid_rf = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

rf_model = RandomForestClassifier(random_state=42)
kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)
grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search_rf.fit(X_train, y_train)
print("Best decision RandomForest tree parameter: ", grid_search_rf.best_params_)

# Evaluate the best RandomForest model
best_rf_model = grid_search_rf.best_estimator_
y_pred_rf = best_rf_model.predict(X_test)

accuracy_rf = accuracy_score(y_test, y_pred_rf)
print(f"Final RandomForest Model Accuracy: {accuracy_rf}")
print("Classification Report for RandomForest:\n", classification_report(y_test, y_pred_rf))

# Calculate training accuracy for RandomForest
y_train_pred_rf = best_rf_model.predict(X_train)
training_accuracy_rf = accuracy_score(y_train, y_train_pred_rf)
print(f"RandomForest Training Accuracy: {training_accuracy_rf}")

# Confusion Matrix for RandomForest
cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Reds', xticklabels=["Not Accepted", "Accepted"], yticklabels=["Not Accepted", "Accepted"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("RandomForest Confusion Matrix")
plt.show()

"""from sklearn import tree
tree.plot_tree(best_rf_model.estimators_[0], feature_names=X.columns, class_names=["Not Accepted", "Accepted"], filled=True)
plt.show()

-------------------------------------------------------------

#PART II : XGBoost
"""

from sklearn.model_selection import  StratifiedKFold, GridSearchCV
# Model training - XGBoost with GridSearchCV
param_grid_xgb = {
    'n_estimators': [100, 200, 300],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.6, 0.8, 1.0],
    'colsample_bytree': [0.6, 0.8, 1.0]
}

xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')
kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)
grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb, cv=kfold, scoring='accuracy', n_jobs=-1, verbose=2)
grid_search_xgb.fit(X_train, y_train)
print("Best decision XGBoost tree parameter: ", grid_search_xgb.best_params_)

# Evaluate the best XGBoost model
best_xgb_model = grid_search_xgb.best_estimator_
y_pred_xgb = best_xgb_model.predict(X_test)
accuracy_xgb = accuracy_score(y_test, y_pred_xgb)
print(f"XGBoost Final Model Accuracy: {accuracy_xgb}")
print("Classification Report:\n", classification_report(y_test, y_pred_xgb))

# Calculate training accuracy for XGBoost model
y_train_pred_xgb = best_xgb_model.predict(X_train)
training_accuracy_xgb = accuracy_score(y_train, y_train_pred_xgb)
print(f"Training Accuracy: {training_accuracy_xgb}")

# Confusion Matrix for XGBoost
cm_xgb = confusion_matrix(y_test, y_pred_xgb)
sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Greens', xticklabels=["Not Accepted", "Accepted"], yticklabels=["Not Accepted", "Accepted"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("XGBoost Confusion Matrix")
plt.show()

"""#Step 5: Evaluation Accuracy & Visualize"""

#PartI .. cntd..

# Model Accuracy Plot for RandomForest
accuracies = [training_accuracy_rf, accuracy_rf]
labels_rf = ["Training Accuracy", "Test Accuracy"]
plt.figure(figsize=(6, 4))
plt.bar(labels_rf, accuracies, color=['orange','red'])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison for RandomForest")
plt.show()

#PartII .. cntd..

# Model Accuracy Plot for XGBoost
accuracies = [training_accuracy_xgb, accuracy_xgb]
labels = ["Training Accuracy", "Test Accuracy"]
plt.figure(figsize=(6, 4))
plt.bar(labels, accuracies, color=['lightgreen','green'])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Model Accuracy Comparison for XGBoost")
plt.show()

# Accuracy Comparison Plot between 'RandomForest' & 'XGBoost'
models = ['RandomForest', 'XGBoost']
accuracies = [accuracy_rf, accuracy_xgb]
plt.figure(figsize=(6, 4))
plt.bar(models, accuracies, color=['red', 'green'])
plt.ylim(0, 1)
plt.ylabel("Accuracy optmized with gridsearchCV ")
plt.title("Model Accuracy Comparison: RandomForest vs. XGBoost")
plt.show()

"""++++++++++++++++++++++ Additional part -- Optimized XGBoost with RandomizedSearchCV ++++++++++++++++++++++

#Step4: Hyperparameter Tunning with  RandomizedSearchCV (rcv)
"""

from sklearn.model_selection import RandomizedSearchCV
import numpy as np

# Define the hyperparameter grid
param_grid_rcv = {
    'n_estimators': [100, 200, 300, 500],  # Number of trees
    'max_depth': [3, 5, 7, 10],  # Maximum depth of each tree
    'learning_rate': [0.01, 0.05, 0.1, 0.2],  # Step size shrinkage
    'subsample': [0.6, 0.8, 1.0],  # Percentage of samples used per tree
    'colsample_bytree': [0.6, 0.8, 1.0],  # Percentage of features used per tree
    'gamma': [0, 0.1, 0.2, 0.3],  # Minimum loss reduction for split
    'reg_alpha': [0, 0.01, 0.1, 1],  # L1 regularization
    'reg_lambda': [0, 0.01, 0.1, 1]  # L2 regularization
}

# Initialize XGBoost model
xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='mlogloss')

# RandomizedSearchCV for hyperparameter tuning
random_search = RandomizedSearchCV(
    estimator=xgb_model,
    param_distributions=param_grid_rcv,
    n_iter=20,  # Number of combinations to try
    scoring='accuracy',
    cv=5,  # 5-fold cross-validation
    verbose=2,
    n_jobs=-1,
    random_state=42
)

!pip install scikit-learn==1.3.0 xgboost --upgrade

# Fit the model
random_search.fit(X_train, y_train)

# Best parameters & best score
print("Best Hyperparameters:", random_search.best_params_)
print("Best Accuracy Score:", random_search.best_score_)

# Train final model with best parameters
best_xgb_model = random_search.best_estimator_

# Predict on test data
y_pred_rcv = best_xgb_model.predict(X_test)

# Evaluate the model
accuracy_rcv = accuracy_score(y_test, y_pred_rcv)
print(f"Final Model Accuracy: {accuracy_rcv}")
print("Classification Report:\n", classification_report(y_test, y_pred_rcv))

#Find Training accuracy
y_train_pred_rcv = best_xgb_model.predict(X_train)
training_accuracy_rcv = accuracy_score(y_train, y_train_pred_rcv)
print(f"Training Accuracy using xgboost RandomizedSearchCV: {training_accuracy_rcv}")

#Calculate confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred_rcv)
print(cm)

# Model Accuracy Plot
accuracies = [training_accuracy_rcv, accuracy_rcv]
labels = ["Training Accuracy", "Test Accuracy"]
plt.figure(figsize=(6, 4))
plt.bar(labels, accuracies, color=['yellow','m'])
plt.ylim(0, 1)
plt.ylabel("Accuracy")
plt.title("Model Accuracy optimized using RandomizedSearchCV")
plt.show()

"""====++++++++++++++============================================================++++++++++======"""

df.columns
df['MostFrequentCategory'].count()
df["MostFrequentCategory"].value_counts()

encoder = label_encoders['MostFrequentCategory']
mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
print(mapping)

"""#offers recommendation"""

#offers recommendation
def recommend_offer(row):
 if row['SpendingCategory']==3 and row['MostFrequentCategory']==1:
     return "Loyalty Points"
 elif row['SpendingCategory']>=2 and row['MostFrequentCategory']==0:
     return "Cashback 10%"
 elif row['SpendingCategory']>=1 and row['MostFrequentCategory']==2:
     return "Discount 10%"
 elif row['SpendingCategory']==0 and row['MostFrequentCategory']==3:
     return "Freebie"
 else :
     return "BOGO"

# Generate target variable
df["predicted_offer"] = df.apply(recommend_offer, axis=1)

df

"""#Step5: Data Visualization"""

#Figure size setup
fig, ax = plt.subplots()

#1. User Spending Distribution
sns.histplot(df["TransactionAmount"], bins=30, kde=True,  color="blue")
ax.set_title("User Spending Distribution (Transaction Amounts)")

plt.figure(figsize=(6, 4))
sns.countplot(x=df["offer_accepted"], palette="coolwarm")
plt.title("Offer Acceptance Rate")
plt.xlabel("Offer Accepted (0=No, 1=Yes)")
plt.ylabel("Count")
plt.show()

#After SMOTE
plt.figure(figsize=(6,4))
sns.countplot(x=y_resampled, palette="coolwarm")
plt.title("Class Distribution After SMOTE")
plt.xlabel("Offer Accepted (0 = No, 1 = Yes)")
plt.ylabel("Count")
plt.show()

category_counts = df["transaction_category_last"].value_counts()
sns.barplot(x=category_counts.index, y=category_counts.values, palette="coolwarm")
plt.tight_layout()
plt.show()

# Convert TransactionDate to extract time-based features
df["TransactionMonth"] = df["TransactionDate"].dt.to_period("M")


plt.figure(figsize=(12, 5))
df.groupby("TransactionMonth").size().plot(kind="line", marker="o", color="brown")
plt.title("Total Transactions Per Month")
plt.xlabel("Month")
plt.ylabel("Transaction Count")
plt.xticks(rotation=45)
plt.show()

"""Visualize"""

# Count the number of users in each spending category
spending_counts = df["SpendingCategory"].value_counts()
spending_counts

encoder = label_encoders["SpendingCategory"]
mapping = dict(zip(encoder.classes_, encoder.transform(encoder.classes_)))
print(mapping)

import matplotlib.pyplot as plt
import seaborn as sns

# Set plot style
sns.set_style("whitegrid")

# Create subplots
fig, axes = plt.subplots(2, 2, figsize=(14, 10))

# 1. Spending Category Distribution
sns.countplot(x=df["SpendingCategory"], palette="coolwarm", ax=axes[0, 0])
axes[0, 0].set_title("Spending Category Distribution")
axes[0, 0].set_xlabel("Spending Category")
axes[0, 0].set_ylabel("Count")

# 2. Most Frequent Transaction Categories
sns.countplot(y=df["MostFrequentCategory"], palette="Blues_r", ax=axes[0, 1])
axes[0, 1].set_title("Most Frequent Transaction Categories")
axes[0, 1].set_xlabel("Count")
axes[0, 1].set_ylabel("Transaction Category")



# 3. Offer Acceptance Rate
sns.countplot(x=df["offer_accepted"], palette="viridis", ax=axes[1, 0])
axes[1, 0].set_title("Offer Acceptance Rate")
axes[1, 0].set_xlabel("Offer Accepted (0 = No, 1 = Yes)")
axes[1, 0].set_ylabel("Count")

# 4. Personalized Offer Recommendations
df["predicted_offer"] = df.apply(recommend_offer, axis=1)
sns.countplot(y=df["predicted_offer"], palette="Set2", ax=axes[1, 1])
axes[1, 1].set_title("Personalized Offer Recommendations")
axes[1, 1].set_xlabel("Count")
axes[1, 1].set_ylabel("Recommended Offer")

plt.tight_layout()
plt.show()

# Calculate offer redemption rate for each spending category
redemption_rates = df.groupby("SpendingCategory")["offer_accepted"].mean() * 100  # Convert to percentage

# Plot the grouped bar chart
plt.figure(figsize=(8, 5))
sns.barplot(x=redemption_rates.index, y=redemption_rates.values, palette="coolwarm")

# Formatting
plt.xlabel("User Spending Category")
plt.ylabel("Offer Redemption Rate (%)")
plt.title("Offer Redemption Rate by Spending Category")
plt.ylim(0, 100)  # Ensure percentage scale
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Show values on bars
for index, value in enumerate(redemption_rates.values):
    plt.text(index, value + 1, f"{value:.1f}%", ha='center', fontsize=12, fontweight='bold')

plt.show()

# Aggregate user engagement data
engagement_data = df.groupby("SpendingCategory").agg(
    AvgTransactionAmount=("TransactionAmount", "mean"),
    OfferRedemptionRate=("offer_accepted", "mean"),
    TransactionFrequency=("TransactionID", "count")
).reset_index()

# Normalize Transaction Frequency for better visualization
engagement_data["TransactionFrequency"] = engagement_data["TransactionFrequency"] / engagement_data["TransactionFrequency"].max()

# Pivot the data for heatmap
engagement_pivot = engagement_data.pivot(index="SpendingCategory", columns="OfferRedemptionRate", values="TransactionFrequency")

# Plot heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(engagement_pivot, cmap="coolwarm", annot=True, fmt=".2f", linewidths=0.5)

# Labels and title
plt.xlabel("Offer Redemption Rate")
plt.ylabel("Spending Category")
plt.title("ðŸ“Š User Engagement Heatmap: Spending vs. Offer Redemption")
plt.show()

# Additional Insights Plots
fig, axes = plt.subplots(2, 2, figsize=(12, 10))

# User Spending Distribution
sns.histplot(df["TransactionAmount"], bins=30, kde=True, color="blue", ax=axes[0, 0])
axes[0, 0].set_title("User Spending Distribution")

# Offer Acceptance Rate per Spending Category
sns.barplot(x=df["SpendingCategory"], y=df['offer_acceptance_rate'], ax=axes[0, 1])
axes[0, 1].set_title("Offer Acceptance Rate by Spending Category")

# Transaction Count per Category
sns.countplot(x=df["MostFrequentCategory"], ax=axes[1, 0])
axes[1, 0].set_title("Transaction Count per Category")

# Average Transaction Amount per Category
sns.barplot(x=df["MostFrequentCategory"], y=df["AvgTransactionAmount"], ax=axes[1, 1])
axes[1, 1].set_title("Avg Transaction Amount per Category")

plt.tight_layout()
plt.show()

#Compute correlation matrix
corr_matrix = df.select_dtypes(include=["number"]).corr()

#Plot heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, annot=True, cmap="coolwarm", fmt=".2f", linewidths=0.5)
plt.title("Feature Correlation Heatmap")
plt.show()